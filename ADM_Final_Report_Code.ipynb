{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas 및 Numpy 패키지 불러오기\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# 국건영 2016년, 2017년, 2018년도 데이터 불러오기\n",
    "df16 = pd.read_sas('hn16_all.sas7bdat')\n",
    "df17 = pd.read_sas('hn17_all.sas7bdat')\n",
    "df18 = pd.read_sas('hn18_all.sas7bdat')\n",
    "\n",
    "df_all = pd.concat([df16,df17,df18])\n",
    "\n",
    "# 선행 연구(논문) 기반 유의미한 변수 추출\n",
    "df_att = df_all[[\"sex\",\"age\",\"D_1_1\",\"Total_slp_wk\",\"BP1\",\"BO1\",\"BE5_1\",\"BE8_1\",\"BE8_2\",\"BP5\",\"HE_ht\",\"HE_wt\"]]\n",
    "\n",
    "#히스토그램 산출 ( 탐색적 분석 )\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "for df in [\"sex\",\"age\",\"D_1_1\",\"Total_slp_wd\",\"BP1\",\"BO1\",\"BE5_1\",\"BE8_1\",\"BE8_2\",\"BP5\",\"HE_ht\",\"HE_wt\"]:\n",
    "    plt.hist(df_att[df])\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title('Histogram of {}'.format(df))\n",
    "    plt.show()\n",
    "\n",
    "# BMI [BMI] (명목) 변수 생성\n",
    "df_att['BMI'] = (df_att['HE_wt'] / (df_att['HE_ht'] / 100)**2)\n",
    "df_att.loc[df_att['BMI'] < 25, 'BMI'] = 0\n",
    "df_att.loc[df_att['BMI'] >= 25, 'BMI'] = 1\n",
    "\n",
    "# 청소년 데이터 생성\n",
    "df_csn = df_att.loc[(df_att['age']>=12) & (df_att['age']<=18), :] )\n",
    "\n",
    "# 하루 평균 앉아서 보내는 시간(분 단위) 변수 생성\n",
    "df_csn[\"SitTime\"] = df_csn[\"BE8_1\"]*60 + df_csn['BE8_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 청소년 건강행태조사 2018년 데이터 불러오기\n",
    "df_cgh = pd.read_csv('cgh18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 국건영, 청건행 데이터 통합\n",
    "df_att = pd.concat([df_csn,a])\n",
    "df_att['BMI'] = df_att['BMI'].astype('category')\n",
    "df_att.loc[(df_att['BE8_1'] == 88) |(df_att['BE8_1'] == 99) , 'BE8_1'] = np.nan \n",
    "df_att.loc[(df_att['BE8_2'] == 88) |(df_att['BE8_2'] == 99) , 'BE8_2'] = np.nan  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요 Attribute만 가져오기\n",
    "df_att = df_att.loc[:,['sex','age','D_1_1','BP1','BO1','BE5_1','BP5','BMI','Total_slp_wk','SitTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1주일간 근력운동 일수 (등간)\n",
    "print(df_att['BE5_1'].value_counts()) # 8 비해당 929명, 9 모름, 무응답 : 267명 -> NA\n",
    "df_att.loc[(df_att['BE5_1'] == 8) |(df_att['BE5_1'] == 9) , 'BE5_1'] = np.nan  #na 값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주관적 건강상태 [D_1_1]] (순서)\n",
    "from pandas.api.types import CategoricalDtype\n",
    "df_att.loc[df_att['D_1_1'] == 9, 'D_1_1'] = np.nan\n",
    "df_att['D_1_1'] = df_att['D_1_1'].astype(CategoricalDtype(ordered=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2주 이상 연속 우울감 여부 [BP5] (명목)\n",
    "df_att.loc[df_att['BP5'] == 8, 'BP5'] = np.nan\n",
    "df_att.loc[df_att['BP5'] == 9, 'BP5'] = np.nan\n",
    "df_att['BP5'] = df_att['BP5'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평소 스트레스 인지정도 [BP1] (순서)\n",
    "from pandas.api.types import CategoricalDtype\n",
    "df_att.loc[df_att['BP1'] == 8, 'BP1'] = np.nan\n",
    "df_att.loc[df_att['BP1'] == 9, 'BP1'] = np.nan\n",
    "df_att['BP1'] = df_att['BP1'].astype(CategoricalDtype(ordered=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주관적 체형 인지 [BO1] (순서)\n",
    "from pandas.api.types import CategoricalDtype)\n",
    "df_att.loc[df_att['BO1'] == 8, 'BO1'] = np.nan\n",
    "df_att.loc[df_att['BO1'] == 9, 'BO1'] = np.nan\n",
    "df_att['BO1'] = df_att['BO1'].astype(CategoricalDtype(ordered=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1주일간 근력운동 일수 [BE5_1]] (순서)\n",
    "from pandas.api.types import CategoricalDtype\n",
    "df_att['BE5_1'] = df_att['BE5_1'].astype(CategoricalDtype(ordered=True))\n",
    "df_att['sex'] = df_att['sex'].astype('category') #성별 범주화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치 처리\n",
    "- 전체(남녀 통합) 데이터셋: df_all\n",
    "- 남자 데이터셋: df_all_b\n",
    "- 여자 데이터셋: df_all_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 제거\n",
    "df_new = df_att.dropna(axis=0)\n",
    "df_new = df_new.reset_index()\n",
    "df_all = df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#남자 데이터 생성\n",
    "df_all_b = df_all.loc[df_all[\"sex\"]==1,:]\n",
    "df_all_b = df_all_b.drop('sex', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#여자 데이터 생성\n",
    "df_all_g = df_all.loc[df_all[\"sex\"]==2,:]\n",
    "df_all_g = df_all_g.drop('sex', axis = 1)\n",
    "df_all = df_all.iloc[:,1:11]\n",
    "df_all_g = df_all_g.iloc[:,1:11]\n",
    "df_all_b = df_all_b.iloc[:,1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이상치 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier 제거 함수 생성\n",
    "def remove_outlier(d_cp, column):\n",
    "    fraud_column_data = d_cp[column]\n",
    "    quan_25 = np.percentile(fraud_column_data, 25)\n",
    "    quan_75 = np.percentile(fraud_column_data, 75)\n",
    "    iqr = quan_75 - quan_25\n",
    "    iqr = iqr * 1.5\n",
    "    lowest = quan_25 - iqr\n",
    "    highest = quan_75 + iqr\n",
    "    outlier_index = fraud_column_data[(fraud_column_data<lowest) | (fraud_column_data > highest)].index\n",
    "    d_cp.drop(outlier_index, axis=0, inplace=True)\n",
    "    return d_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier 제거 실행\n",
    "df_all = remove_outlier(df_all, 'SitTime')\n",
    "df_all = remove_outlier(df_all, 'Total_slp_wk')\n",
    "df_all_g = remove_outlier(df_all_g, 'SitTime')\n",
    "df_all_g = remove_outlier(df_all_g, 'Total_slp_wk')\n",
    "df_all_b = remove_outlier(df_all_b, 'SitTime')\n",
    "df_all_b = remove_outlier(df_all_b, 'Total_slp_wk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test 분할, 업샘플링, 스케일링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train_test 나누기와 업샘플링 그리고 scaling 적용 여부 함수 생성\n",
    "def train_test_split_and_upsample(df_input, scaling):\n",
    "    df = df_input.copy()\n",
    "    df = df.astype('category')\n",
    "    df['SitTime'] = df['SitTime'].astype('float64')\n",
    "    df['Total_slp_wk'] = df['Total_slp_wk'].astype('float64')\n",
    "    df['age'] = df['age'].astype('category')\n",
    "    df_class_0 = df[df['BMI'] == 0]\n",
    "    df_class_1 = df[df['BMI'] == 1]\n",
    "    df_class_1_over = df_class_1.sample(df_class_0.shape[0], replace=True, random_state=10)\n",
    "    df_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n",
    "    feature_columns = list(df.columns.difference(['BMI']))\n",
    "    X = df[feature_columns]\n",
    "    y = df[['BMI']]\n",
    "    X_res = df_over[feature_columns]\n",
    "    y_res = df_over[['BMI']\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn import preprocessing\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y) \n",
    "    train_x_res, test_x_res, train_y_res, test_y_res = train_test_split(X_res, y_res, test_size = 0.3, random_state = 0, stratify = y_res)   #test train set 나누기\n",
    "    train_x = train_x.reset_index().iloc[:,1:]\n",
    "    tmp2 = train_x.loc[:,['Total_slp_wk', 'SitTime']]\n",
    "    train_x = train_x.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "    train_x = pd.concat([train_x, tmp2], axis=1)\n",
    "    test_x = test_x.reset_index().iloc[:,1:]\n",
    "    tmp2 = test_x.loc[:,['Total_slp_wk', 'SitTime']]\n",
    "    test_x = test_x.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "    test_x = pd.concat([test_x, tmp2], axis=1)\n",
    "    train_y = train_y.reset_index()['BMI']\n",
    "    test_y = test_y.reset_index()['BMI']\n",
    "    train_x_res = train_x_res.reset_index().iloc[:,1:]\n",
    "    tmp2 = train_x_res.loc[:,['Total_slp_wk', 'SitTime']]\n",
    "    train_x_res = train_x_res.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "    train_x_res = pd.concat([train_x_res, tmp2], axis=1)\n",
    "    train_y_res = train_y_res.reset_index()['BMI']\n",
    "    #sacling 파라미터 지정\n",
    "    if scaling == True:\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        temp = min_max_scaler.fit_transform(train_x.loc[:,['Total_slp_wk','SitTime']])\n",
    "        temp = pd.DataFrame(temp, columns = ['Total_slp_wk','SitTime'])\n",
    "        train_x = train_x.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "        train_x = pd.concat([train_x, temp], axis=1)\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        temp = min_max_scaler.fit_transform(test_x.loc[:,['Total_slp_wk','SitTime']])\n",
    "        temp = pd.DataFrame(temp, columns = ['Total_slp_wk','SitTime'])\n",
    "        test_x = test_x.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "        test_x = pd.concat([test_x, temp], axis=1)\n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        temp = min_max_scaler.fit_transform(train_x_res.loc[:,['Total_slp_wk','SitTime']])\n",
    "        temp = pd.DataFrame(temp, columns = ['Total_slp_wk','SitTime'])\n",
    "        train_x_res = train_x_res.drop(['Total_slp_wk', 'SitTime'], axis=1)\n",
    "        train_x_res = pd.concat([train_x_res, temp], axis=1)    \n",
    "    return train_x, test_x, train_y, test_y, train_x_res, train_y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오버 샘플링 데이터 출력\n",
    "#df_all 대신 df_all_b, df_all_g를 넣으면 걔네들에 대해서 트레인, 테스트, 오버샘플링 출력\n",
    "tmp = train_test_split_and_upsample(df_all, scaling=True) \n",
    "train_x = tmp[0]\n",
    "train_y = tmp[2]\n",
    "test_x = tmp[1]\n",
    "test_y = tmp[3]\n",
    "train_x_res = tmp[4]\n",
    "train_y_res = tmp[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 및 변수 중요도 파악"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#내가 원하는 기존train또는 oversample된 train을 구하기\n",
    "\n",
    "#train_x = train_x_res.copy()\n",
    "#train_y = train_y_res.copy()\n",
    "\n",
    "train_x = make_pca_col(train_x_res)\n",
    "train_y = train_y_res.copy()\n",
    "test_x = make_pca_col(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF 그리드서치 \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "#rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\n",
    "#rf.fit(X_train, y_train)\n",
    "\n",
    "#최적 파라미터 값 찾기\n",
    "params = { 'n_estimators' : [10, 100],\n",
    "           'max_depth' : [6, 8, 10, 12],\n",
    "           'min_samples_leaf' : [8, 12, 18],\n",
    "           'min_samples_split' : [8, 16, 20]\n",
    "            }\n",
    "rf_clf = RandomForestClassifier(random_state = 0, n_jobs = -1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid = params, cv = 10, n_jobs = -1, scoring='recall')\n",
    "grid_cv.fit(train_x, train_y)\n",
    "\n",
    "predicted = grid_cv.predict(test_x)\n",
    "predicted_proba = grid_cv.predict_proba(test_x)\n",
    "\n",
    "pred_y_proba = []\n",
    "pred_y_proba_list = grid_cv.predict_proba(test_x)\n",
    "for i in range(pred_y_proba_list.shape[0]):\n",
    "    pred_y_proba.append(pred_y_proba_list[i][1])\n",
    "\n",
    "# AUC\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, pred_y_proba)\n",
    "\n",
    "# Print ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "# Print AUC\n",
    "auc = np.trapz(tpr,fpr)\n",
    "\n",
    "print(confusion_matrix(test_y, predicted))\n",
    "print(model_evaluation(test_y, predicted))\n",
    "print('AUC:', round(auc,3))\n",
    "plt.plot(fpr,tpr)\n",
    "print('최적 하이퍼 파라미터: ', grid_cv.best_params_)\n",
    "#print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수중요도 추출1\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance \n",
    "\n",
    "perm = PermutationImportance(grid_cv, random_state = 0).fit(train_x, train_y) \n",
    "df_impt = eli5.show_weights(perm, top = 80, feature_names = train_x.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수중요도 추출2\n",
    "df_impt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수중요도 시각화\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "result = permutation_importance(grid_cv, train_x, train_y, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "plt.barh(train_x.columns[sorted_idx], sorted(result.importances_mean))\n",
    "plt.title('Permutation Importance', fontsize=18)\n",
    "plt.ylabel('Feature name', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 공선성 확인용 코드 \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "데이터프레임 = add_constant(데이터프레임)\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(\n",
    "    데이터프레임.values, i) for i in range(데이터프레임.shape[1])]\n",
    "vif[\"features\"] = 데이터프레임.columns\n",
    "vif.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Measure 출력 함수 만들기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    F2_Score = (5 * Recall * Precision) / (Recall + 4*Precision) # Recall을 Precision보다 2배 중요하게 생각하여 F2 Score 사용\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy) \n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"Specificity: \", Specificity)\n",
    "    print(\"F1_Score: \", F1_Score)\n",
    "    print(\"F2_Score: \", F2_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-평균 군집을 사용한 전처리 ==> 로지스틱 회귀\n",
    "# 이 블록은 K-평균에서의 최적의 K값 찾는 것. \n",
    "\n",
    "param_grid = dict(kmeans__n_clusters = range(95,105+1))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     (\"kmeans\", KMeans()),  # \n",
    "                     (\"log_reg\", LogisticRegression(max_iter=15000)),     \n",
    "])\n",
    "\n",
    "grid_clf = GridSearchCV(pipeline, param_grid)\n",
    "grid_clf.fit(train_x_res, train_y_res.values.ravel())  # A column-vector y was passed when a 1d array was expected라는 에러가 떠서 .values.ravel() 이용해서 column vector를 1d array로 형태 변환해줌\n",
    "print(grid_clf.best_params_)\n",
    "print(grid_clf.score(test_x, test_y.values.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Means 전처리하지 않고 그냥 그리드 서치 적용 및 l1 norm, l2 norm\n",
    "\n",
    "# 파라미터 후보\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2']}\n",
    "\n",
    "# 그리드 서치 진행\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', random_state = 42), param_grid, cv= 2 )        \n",
    "\n",
    "grid_search.fit(train_x, train_y.values.ravel())\n",
    "grid_search.score(test_x, test_y.values.ravel())\n",
    "print(\"best parameters : {}\".format(grid_search.best_params_))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = grid_search.predict(test_x)\n",
    "print(confusion_matrix(test_y.values.ravel(), predicted))\n",
    "print(model_evaluation(test_y.values.ravel(), predicted))\n",
    "print(\"AUC: \", roc_auc_score(test_y.values.ravel(), np.round(predicted,0)))\n",
    "print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Means 전처리하지 않고 그냥 그리드 서치 적용 및 ElasticNet 사용 시\n",
    "\n",
    "# 파라미터 후보\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10], 'l1_ratio' : [0.3, 0.6, 0.9]}\n",
    "\n",
    "# 그리드 서치 진행\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='saga', random_state = 42,  max_iter = 5000, penalty= 'elasticnet'), param_grid, cv= 2)        \n",
    "\n",
    "grid_search.fit(train_x, train_y.values.ravel())\n",
    "grid_search.score(test_x, test_y.values.ravel())\n",
    "print(\"best parameters : {}\".format(grid_search.best_params_))\n",
    "        \n",
    "predicted = grid_search.predict(test_x)\n",
    "print(confusion_matrix(test_y.values.ravel(), predicted))\n",
    "print(model_evaluation(test_y.values.ravel(), predicted))\n",
    "print(\"AUC: \", roc_auc_score(test_y.values.ravel(), np.round(predicted,0)))\n",
    "\n",
    "#print('최고 예측 정확도: {:.4f}'.format(grid_cv.best_score_))\n",
    "print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Means 전처리 후 그리드 서치 적용 및 l1 norm, l2 norm 사용 시\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'log_reg__C': [0.01, 0.1, 1, 10],\n",
    "    'log_reg__penalty' : ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     (\"kmeans\", KMeans(n_clusters= 122 )),  # \n",
    "                     (\"log_reg\", LogisticRegression(solver='liblinear', max_iter = 20000)),     # 이 때 logistic Regression 안에 파라미터 넣던가 아니면 LogistricRegression 대신 GridSearCV를 쓰는 것도 시도해보기 \n",
    "])\n",
    "\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv = 5)\n",
    "grid_clf.fit(train_x_res, train_y_res.values.ravel())  # A column-vector y was passed when a 1d array was expected라는 에러가 떠서 .values.ravel() 이용해서 column vector를 1d array로 형태 변환해줌\n",
    "print(\"best parameters : \", grid_clf.best_params_)\n",
    "print(\"Grid Search Score: \", grid_clf.score(test_x, test_y.values.ravel()))\n",
    "\n",
    "predicted = grid_clf.predict(test_x)\n",
    "print(confusion_matrix(test_y.values.ravel(), predicted))\n",
    "print(model_evaluation(test_y.values.ravel(), predicted))\n",
    "print(\"AUC: \", roc_auc_score(test_y.values.ravel(), np.round(predicted,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K Means 전처리 후 그리드 서치 적용 및 ElasticNet 사용 시\n",
    "\n",
    "param_grid = {\n",
    "    'log_reg__C': [ 0.01, 0.1, 1, 10],\n",
    "    'log_reg__l1_ratio' : [0.3, 0.6, 0.9],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                     (\"kmeans\", KMeans(n_clusters= 2 )),  # \n",
    "                     (\"log_reg\", LogisticRegression(solver='saga', penalty = 'elasticnet', max_iter = 20000)),     # 이 때 logistic Regression 안에 파라미터 넣던가 아니면 LogistricRegression 대신 GridSearCV를 쓰는 것도 시도해보기 \n",
    "])\n",
    "\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv = 5)\n",
    "grid_clf.fit(train_x_res, train_y_res.values.ravel())  # A column-vector y was passed when a 1d array was expected라는 에러가 떠서 .values.ravel() 이용해서 column vector를 1d array로 형태 변환해줌\n",
    "print(\"best parameters : \", grid_clf.best_params_)\n",
    "print(\"Grid Search Score: \", grid_clf.score(test_x, test_y.values.ravel()))\n",
    "\n",
    "predicted = grid_clf.predict(test_x)\n",
    "print(confusion_matrix(test_y, predicted))\n",
    "print(model_evaluation(test_y, predicted))\n",
    "print(\"AUC: \", roc_auc_score(test_y, np.round(predicted,0)))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(C = 1, penalty = 'l1', solver = 'liblinear', random_state = 42)\n",
    "logreg.fit(train_x,train_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀계수 확인\n",
    "coef_dict = {}\n",
    "for coef, feat in zip(logreg.coef_[0,:],train_x.columns):\n",
    "    coef_dict[feat] = coef\n",
    "\n",
    "coef_dict\n",
    "\n",
    "# 회귀계수 확인 + sklearn에서는 회귀계수를 검정하는 방법이 아직 나오지 않았기 때문에 statsmodels 패키지를 이용하여 설명변수의 p-value만 참고하기\n",
    "# 단 이 코드의 결과는 선형 회귀분석 함수 (OLS)를 이용한 것이기 때문에 회귀계수는 참고하지 않고 p-value만 확인하는 용도로 사용. \n",
    "# 참고 사이트 : https://qastack.kr/programming/27928275/find-p-value-significance-in-scikit-learn-linearregression\n",
    "import statsmodels.api as sm\n",
    "X2 = sm.add_constant(train_x)\n",
    "model = sm.OLS(train_y, X2)\n",
    "result = model.fit()\n",
    "print(result.summary())\n",
    "df_all = df_all.drop(['Total_slp_wd_standard','SitTime_standard','Total_slp_wd_scaled','SitTime_scaled'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    F2_Score = (5 * Recall * Precision) / (Recall + 4*Precision)\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"Specificity: \", Specificity)\n",
    "    print(\"F1-Score: \", F1_Score)\n",
    "    print(\"F2-Score: \", F2_Score)\n",
    "    print(\"auc score: \" , roc_auc_score(label, np.round(predict,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "#최적 파라미터 값 찾기\n",
    "param_grid = [ { 'C' : [0.1, 1, 10 ], 'kernel': [ 'rbf' ], 'gamma' : [ 1,0.1 ]},\n",
    "{ 'C' : [0.1, 1, 10 ], 'kernel': [ 'poly' ], 'gamma' : [ 1,0.1 ]},\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(train_x, train_y,test_x, test_y,cv):\n",
    "    print(\"데이터셋 : \", train_x ,\" &  cv: \" , cv)\n",
    "    grid_search = GridSearchCV(SVC(),param_grid, cv=cv, return_train_score = True)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    print(\"best parameters : {}\".format(grid_search.best_params_))\n",
    "    predicted = grid_search.predict(test_x)\n",
    "    print(confusion_matrix(test_y, predicted))\n",
    "    print(model_evaluation(test_y, predicted))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전체\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#남\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#여\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#전체\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)\n",
    "#남\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)\n",
    "#여\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형SVM\n",
    "\n",
    "param_grid = [ { 'C' : [0.1, 1, 10,100, 1000 ], 'kernel': [ 'linear' ]}\n",
    "\n",
    "def svm_model(train_x, train_y,test_x, test_y,cv):\n",
    "    print(\"데이터셋 : \", train_x ,\" &  cv: \" , cv)\n",
    "    grid_search = GridSearchCV(SVC(),param_grid, cv=cv, return_train_score = True)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "    print(\"best parameters : {}\".format(grid_search.best_params_))\n",
    "    predicted = grid_search.predict(test_x)\n",
    "    print(confusion_matrix(test_y, predicted))\n",
    "    print(model_evaluation(test_y, predicted))\n",
    "\n",
    "#전체\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#남\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#여\n",
    "svm_model(train_x, train_y, test_x, test_y, 5)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 5)\n",
    "#전체\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)\n",
    "#남\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)\n",
    "#여\n",
    "svm_model(train_x, train_y, test_x, test_y, 10)\n",
    "svm_model(train_x_res, train_y_res, test_x, test_y, 10)\n",
    "\n",
    "# 가장 높은 파라미터 : C=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 중요도 확인\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "def feature_plot(classifier, feature_names, top_features):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    plt.figure(figsize=(18, 7))\n",
    "    colors = ['green' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1 + 2 * top_features), feature_names[top_coefficients], rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "lienearsvm = svm.LinearSVC(C=1000).fit(train_x_res, train_y_res)\n",
    "feature_plot(lienearsvm, value,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow, keras 라이브러리 호출\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#입력층\n",
    "input_ = keras.layers.Input(shape=train_x.shape[1:])\n",
    "#은닉층\n",
    "hidden1 = keras.layers.Dense(30, activation = 'relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation = 'relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_,hidden2])\n",
    "#출력층\n",
    "output = keras.layers.Dense(1, activation='sigmoid')(concat)\n",
    "model = keras.Model(inputs = [input_], outputs=[output])\n",
    "#모델 컴파일\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "#early stopping 추가\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights = True)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 훈련\n",
    "model.fit(np.array(train_x.astype(float)), train_y, epochs=100, validation_data = (np.array(test_x.astype(float)), test_y), callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test set 예측\n",
    "preds = model.predict(test_x.astype(float))\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#성능 측정\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    Specificity = cf_matrix[0][0] / (cf_matrix[0][0] + cf_matrix[0][1])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    F2_Score = (5 * Recall * Precision) / (Recall + 4*Precision)\n",
    "    \n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"Specificity: \", Specificity)\n",
    "    print(\"F1-Score: \", F1_Score)\n",
    "    print(\"F2-Score: \", F2_Score)\n",
    "    print(\"auc score: \" , roc_auc_score(label, np.round(predict,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation(test_y, np.round(preds,0))\n",
    "value = train_x_res.columns.values"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
